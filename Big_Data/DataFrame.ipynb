{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Read/Write File DataFrame Operations"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fundamentals\n\n### While reading  \n\n## Use below parameters if required; not mandatory to use\n# If want to read from local user then use \"file:///path\", if want to read from hadoop then use either \"hdfs:///path\" or \"/path\"\n# Use option('inferSchema','true') or inferSchema=True when datatype required according to data in file\n# Use option('header','True') or header=True when first row required as columns of dataframe\n# Use option('delimiter',';') or sep=';' when file contains delimiter to seperate columns, in this case ';', default is ','\n\n### While writing  \n\n## Use below parameters if required; not mandatory to use\n# For CSV uncompressed is default, we can use the following: snappy/uncompressed/gzip/lz4/bzip2\n# For JSON uncompressed is default, we can use the following: snappy/uncompressed/gzip/lz4\n# For ORC snappy is default, we can use the following: snappy/uncompressed/gzip\n# For Parquet snappy is default, we can use the following: snappy/uncompressed/gzip\n# For Avro snappy is default, we can use the following: snappy/uncompressed/deflate\n# Use coalesce(n) when asked to store file in specific partitions; 3 is default (use repartition(n) in case coalesce doesn't work)\n# Use option('compression','snappy') or compression='snappy' when asked to compress file; in this case snappy compression\n# Use option('delimiter','\\t') or sep='\\t' to seperate each column of file; in this case file is tab delimited; default is ','\n# Use option('mode','overwrite') or mode='overwrite' when required to overwrite the existing file, 'error' is default; mode = overwrite/append/ignore/error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CSV File"},{"metadata":{},"cell_type":"markdown","source":"### Read CSV File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').format(\"csv\").load(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.csv(\"/path\", header=True, inferSchema=True, sep=\",\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Write CSV File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.coalesce(4).write.format(\"csv\").option('delimiter',';').option(\"compression\",\"snappy\").option('mode','overwrite').save(\"/path\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.coalesce(4).write.csv(\"/path\", compression=\"snappy\", mode=\"overwrite\", sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## JSON File"},{"metadata":{},"cell_type":"markdown","source":"### Read JSON File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').format(\"json\").load(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').json(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Write JSON File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.write.format('json').option('delimiter',';').option(\"compression\",\"snappy\").option('mode','overwrite').save('/path')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.coalesce(4).write.option('delimiter','|').json(\"/path\", compression=\"snappy\", mode=\"overwrite\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ORC File"},{"metadata":{},"cell_type":"markdown","source":"### Read ORC File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').format(\"orc\").load(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').orc(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Write ORC File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.write.format('orc').option('delimiter',';').option(\"compression\",\"snappy\").option('mode','overwrite').save('/path')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.coalesce(4).write.option('delimiter','|').orc(\"/path\", compression=\"snappy\", mode=\"overwrite\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parquet File"},{"metadata":{},"cell_type":"markdown","source":"### Read Parquet File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').format(\"parquet\").load(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').parquet(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Write Parquet File"},{"metadata":{},"cell_type":"markdown","source":"#### Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.write.format('parquet').option('delimiter',';').option(\"compression\",\"snappy\").option('mode','overwrite').save('/path')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.coalesce(4).write.option('delimiter','|').parquet(\"/path\", compression=\"snappy\", mode=\"overwrite\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Avro File"},{"metadata":{},"cell_type":"markdown","source":"### Read Avro File"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.option('inferSchema','True').option('header','True').option('delimiter',';').format(\"avro\").load(\"/path\")\ndf.show()        # Confirm the dataframe read\ndf.printSchema() # Verify the schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Write Avro File"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.write.format('avro').option('delimiter',';').option(\"compression\",\"snappy\").option('mode','overwrite').save('/path')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hive ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Read From Hive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Method 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.format('hive').table('database_name.table_name') # If database is default just use table_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Method 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.table('database_name.table_name') # If database is default just use table_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Write To Hive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Method 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.repartition(5).write.format('parquet').option('compression','snappy').saveAsTable('database_name.table_name', mode='overwrite')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Method 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.write.insertInto('database_name.table_name', overwrite=True) \n# Table must exist before using insertInto where as saveAsTable (method 1) will create a permanent table in hive and load data\n# insertInto will append the data if table already exists on contrary saveAsTable will error if table exists\n# If want to insert data into existing table using parameter saveAsTable then use with mode='append' because default is mode='overwrite'","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}