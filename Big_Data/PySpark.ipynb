{"cells":[{"metadata":{},"cell_type":"markdown","source":"## PySpark Commands"},{"metadata":{},"cell_type":"markdown","source":"### Launch PySpark"},{"metadata":{},"cell_type":"markdown","source":"#### Launch PySpark with Yarn"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyspark --master yarn\n        \n# In case you have two versions of Spark launch it with \"pyspark2 --master yarn\" for spark2.x version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Launch PySpark with Avro Package"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyspark --master yarn \\\n--packages org.apache.spark:spark-avro_2.11:2.4.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Launch PySpark with Avro Package and MySQL connectivity"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyspark --master yarn \\\n--packages org.apache.spark:spark-avro_2.11:2.4.0 \\\n--jars /usr/share/java/mysql-connector-java.jar \\\n--driver-class-path /usr/share/java/mysql-connector-java.jar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Spark Sessioin"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataFrame Operations"},{"metadata":{},"cell_type":"markdown","source":"#### Create data frame from RDD"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.createDataFrame(rdd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Structure of dataframe in summary format"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Print columns and datatype of dataframe in tree format"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get number of records"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preview records"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show(n)  \n\n# Pass the value of n to see the records or use df.show() default value is 20\n\n# Another way is\n\ndf.show(5,False) \n\n# 5 is the value to dislay the number of rows\n# False is to indicate truncate = False, this will display full content of column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert data into array"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Register dataframe as temporary table and perform query against it"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.createTempView('view_name')                # This will create a Temporary View\n\ndf.createOrReplaceTempView('view_name')       # This will create and replace if Temporary View already exists \n \n# These views are temporary which has existance in hive, once the session is closed views will be gone\n\nnew_df = spark.sql('select * from view_name') # Perform queries against the created view\n\nnew_df.show()                                 # Display the results of query  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change all column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df.toDF('column_name_1', 'column_name_2')   # In this case data frame had two columns\n\nnew_df.printSchema()                                 # Verfiy the changes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change specific column name"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df.withColumnRenamed('old_column_name','new_column_name')\n\nnew_df.printSchema()                                 # Verfiy the changes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Add new column from old column"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import *                  # For some transformations we need to import the library\n\nnew_df = df.withColumn('new_column', substring(df.old_column, 1, 6)) # Here we added new column by transforming existing column\n\nnew_df.printSchema()                                 # Verfiy the changes in structure of data frame\n\nnew_df.show()                                        # Confirm the records by displaying","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change datatype of column"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df.withColumn('column_name', df.column_name.cast('int'))  # Changing column data type into integer\n\n#Or\n\nfrom pyspark.sql.types import IntegerType\nnew_df = dataframe.withColumn(\"column_name\", dataframe.column_name.cast(IntegerType()))\n\n#Or\n\nfrom pyspark.sql.types import IntegerType\nnewDF = dataframe.withColumn(\"column_name\", dataframe.column_name.cast(IntegerType))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Configures the number of partitions when shuffling data for joins or aggregations"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.conf.set(\"spark.sql.shuffle.partitions\", \"300\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Set file compression"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.conf.set(\"spark.sql.parquet.compression.codec\",\"snappy\")  # This will set the compression for a spark session\n\nspark.conf.set(\"spark.file.orc.codec\",\"snappy\")                 # This will set the compression for orc files\n                                                                # Try replacing the file name in the conf for different files","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Date Formats"},{"metadata":{"trusted":true},"cell_type":"code","source":"Sample Output                 MySQL                             Oracle                                 Spark\n\n2013-02-14          DATE_FORMAT(NOW(), '%Y-%m-%d')      TO_CHAR(SYSDATE, 'YYYY-MM-DD')          DATE_FORMAT(NOW(), 'y-MM-dd')\n\n14/02/13            DATE_FORMAT(NOW(), '%d/%m/%y'       TO_CHAR(SYSDATE, 'DD/MM/RR')            DATE_FORMAT(NOW(), 'dd/MM/yy')\n                                \n14-February-13      DATE_FORMAT(NOW(), '%d-%M-%y')      TO_CHAR(SYSDATE, 'DD-MONTH-RR')         DATE_FORMAT(NOW(), 'yyyy-MMMM-dd-E')\n                                                                                                                    2011-March-22-Thu\n14/02/13 15:35:22   DATE_FORMAT(NOW(), '%d/%m/%y %T')   TO_CHAR(SYSDATE, 'DD/MM/RR HH24:MI:SS') DATE_FORMAT(NOW(), 'dd/MM/yy HH:mm:ss')\n                                                                                                                             hh - 12 hr\n                                                                                                                             HH - 24 hr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change date from bigint to date format (like if we import data through sqoop in avro file and create dataframe with that file then date is in bigint)"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.sql('select to_date(from_unixtime(cast(date_column/1000 as int))) as column_name').show()\n\n# Date alternative for this case\ndate_format(from_unixtime(cast(date_column/1000 as int)),'dd-MM-yyyy')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}