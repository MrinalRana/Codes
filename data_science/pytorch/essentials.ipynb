{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# if numpy and torch are not already install then install them using command in cmd; \n",
    "# pip install numpy\n",
    "# for pytorch get the command from https://pytorch.org/ according to the preferences needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the pytorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 7]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([3,5,7])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 7], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# converting numpy array to tensor\n",
    "# The returned tensor and numpy array share the same memory\n",
    "t = torch.from_numpy(a)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update tensor([3, 5, 7], dtype=torch.int32)\n",
      "After update [ 3  5 11]\n",
      "[ 3  5 11]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before update {t}\")\n",
    "t[2] = 11\n",
    "print(f\"After update {a}\")\n",
    "# \"a\" is also updated as both shares same memory location\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  5 11  9]\n",
      "tensor([ 3,  5, 11], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# appending to array\n",
    "a = np.append(a, 9)\n",
    "print(a)\n",
    "# \"t\" tensor won't be updated, to update we again have to convert it into tensor \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  5, 11,  9], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(a)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# making array with zero values and describing the datatype of elements\n",
    "b = torch.zeros(3,3,dtype=torch.float64)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# building manual dimension array with 3 rows and 5 columns\n",
    "x = torch.zeros([3,5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([3, 5])\n",
      "y = tensor([7, 9])\n",
      "x*y = tensor([21, 45])\n",
      "x.mul(y) = tensor([21, 45])\n",
      "x.mul(y).sum() = 66\n",
      "x.matmul(y) = 66\n",
      "x@y = 66\n"
     ]
    }
   ],
   "source": [
    "# multiplication of two array\n",
    "x = torch.tensor([3,5])\n",
    "y = torch.tensor([7,9])\n",
    "print(f'x = {x}')\n",
    "print(f'y = {y}')\n",
    "print(f'x*y = {x*y}')\n",
    "print(f'x.mul(y) = {x.mul(y)}')\n",
    "print(f'x.mul(y).sum() = {x.mul(y).sum()}')\n",
    "# matmul and @ multiply and sum both arrays\n",
    "print(f'x.matmul(y) = {x.matmul(y)}')\n",
    "print(f'x@y = {x@y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of array\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2218, 0.0072, 0.0434, 0.2280, 0.5328],\n",
      "        [0.9001, 0.2510, 0.7513, 0.1944, 0.8215]])\n"
     ]
    }
   ],
   "source": [
    "# random values of array with 2x5 dimension\n",
    "y = torch.rand(2,5)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2218, 0.0072],\n",
       "        [0.0434, 0.2280],\n",
       "        [0.5328, 0.9001],\n",
       "        [0.2510, 0.7513],\n",
       "        [0.1944, 0.8215]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping the array\n",
    "y.view([5,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the documentation **[here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)** know more about torch.tensor **[here](https://pytorch.org/docs/stable/tensors.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the index of a currently selected device.\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context-manager that changes the selected device.\n",
    "torch.cuda.device(device)\n",
    "# device (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of GPUs available.\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the name of a device.\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a bool indicating if CUDA is currently available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can specify if wants to run on CPU or GPU\n",
    "\n",
    "# For CPU\n",
    "device = torch.device(\"cpu\")\n",
    "# For GPU\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.cuda **[documentation](https://pytorch.org/docs/stable/cuda.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If perfroming complex operations should use GPU power, or if operations are not too heavy we have to adjust learning rate and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction - Using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining training data and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_train and out_train hold the datasets \n",
    "# in_train contains the input data and out_train contains the output data\n",
    "# output data is 3 times of input data\n",
    "in_train = np.array([2,3,4,5], dtype=np.float32)\n",
    "out_train = np.array([6,9,12,15], dtype=np.float32)\n",
    "\n",
    "# intializing weight\n",
    "w = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward method\n",
    "# weights will be multiplied by the input data(in_train)\n",
    "def forward(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case loss calculation is done with MSELoss formula i.e. 1/N(y_pred - y_train)**2\n",
    "\n",
    "def loss(y_train, y_pred):\n",
    "    return ((y_pred - y_train)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MSE (Mean Square Error) which is \n",
    "\n",
    "def gradient(x, y_pred, y_train):\n",
    "    # return np.dot(2*x, (y_pred - y_train)).mean()    # varient of below line\n",
    "    return ( 2*x *(y_pred - y_train)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(11): 0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction before training: f(11): {forward(11):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: w = 2.75806, loss = 1.4828460217\n",
      "epoch 15: w = 2.98049, loss = 0.0096441433\n",
      "epoch 23: w = 2.99843, loss = 0.0000627254\n",
      "epoch 31: w = 2.99987, loss = 0.0000004078\n",
      "epoch 39: w = 2.99999, loss = 0.0000000027\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epoch = 40\n",
    "\n",
    "for i in range(epoch):\n",
    "    # prediction = forward pass\n",
    "    y_pred_data = forward(in_train)\n",
    "    \n",
    "    # loss\n",
    "    ls = loss(out_train, y_pred_data)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(in_train, y_pred_data, out_train)\n",
    "    \n",
    "    # update weights\n",
    "    w = w - (learning_rate * dw)\n",
    "    \n",
    "    # printing the data as it goes\n",
    "    if (i+1) % 8 == 0:\n",
    "        print(f\"epoch {i}: w = {w:.5f}, loss = {ls:.10f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: f(11): 33.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction after training: f(11): {forward(11):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction - Using Tensor and Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "#### tensor:\n",
    "PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype.A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors.\n",
    "If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "#### autograd:\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "Automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining training data and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor([8,9,10,11,12], dtype=torch.float32)\n",
    "output_data = torch.tensor([32,36,40,44,48], dtype=torch.float32)\n",
    "\n",
    "# Have to use requires_grad=True when need to calculate derivative/gradient with respect to varible\n",
    "weight = torch.tensor(0, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x * weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y):\n",
    "    return ((y_pred - y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(13): 0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction before training: f(13): {forward(13):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, weight = 0.00000, loss = 1632.00000\n",
      "epoch: 6, weight = 2.72172, loss = 166.66740\n",
      "epoch: 11, weight = 3.59150, loss = 17.02085\n",
      "epoch: 16, weight = 3.86946, loss = 1.73825\n",
      "epoch: 21, weight = 3.95828, loss = 0.17752\n",
      "epoch: 26, weight = 3.98667, loss = 0.01813\n",
      "epoch: 31, weight = 3.99574, loss = 0.00185\n",
      "epoch: 36, weight = 3.99864, loss = 0.00019\n",
      "epoch: 41, weight = 3.99956, loss = 0.00002\n",
      "epoch: 46, weight = 3.99986, loss = 0.00000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "epoch = 50\n",
    "\n",
    "for i in range(epoch):\n",
    "    output_pred = forward(input_data)\n",
    "    ls = loss(output_pred, output_data)\n",
    "    \n",
    "    # This is Autograd; calculates the gradient/derivative automatically\n",
    "    ls.backward()\n",
    "    if i % 5 == 0:\n",
    "        print(f\"epoch: {i+1}, weight = {weight:.5f}, loss = {ls:.5f}\")\n",
    "    # As the derivative has been calculted with autograd method we can get the value with weight.grad\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad() because weights have requires_grad=True, \n",
    "    # but we don't need to track this in autograd.\n",
    "    # If don't want to wrap the then simply use: weight.data = weight.data - learning_rate * weight.grad\n",
    "    with torch.no_grad():\n",
    "        weight -= (learning_rate * weight.grad) # or use weight.copy_(weight - (learning_rate * weight.grad))\n",
    "        # Manually zero the gradients after updating weights\n",
    "        weight.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(13): 51.999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction before training: f(13): {forward(13):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our own **Custom autograd function** by defining class having constructor and forward method, for details see **[here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-defining-new-autograd-functions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn and optim module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn module: \n",
    "The nn package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
    "\n",
    "#### optim module: \n",
    "The optim package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms. Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters (with torch.no_grad() or .data to avoid tracking history in autograd). This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining training data and test tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here data is taken in 2D shape as later we are modeling data with Linear transformation \n",
    "# where only the feature numbers (column numbers of matrix) will be passed\n",
    "input_data = torch.tensor([[3],[4],[5],[6],[7]], dtype=torch.float)\n",
    "output_data = torch.tensor([[6],[8],[10],[12],[14]], dtype=torch.float)\n",
    "# This is the tensor which we going to predict\n",
    "test = torch.tensor([15], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Data using nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies a linear transformation to the incoming data: y = x*W^T + b\n",
    "\n",
    "in_feature = input_data.shape[1] # returns column numbers or use num_rows,num_cols=input_data.shape\n",
    "out_feature = output_data.shape[1]\n",
    "\n",
    "# Replacing the manual forward method, weights and bais\n",
    "model = nn.Linear(in_feature, out_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand nn.Linear go to stackflow or **[click here](https://stackoverflow.com/questions/54916135/what-is-the-class-definition-of-nn-linear-in-pytorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(15) = -12.29544\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction before training: f(15) = {model(test).item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 50, weight = 1.9289791584014893, loss = 0.010838186368346\n",
      "epoch = 100, weight = 1.9338905811309814, loss = 0.009390989318490\n",
      "epoch = 150, weight = 1.938462495803833, loss = 0.008137014694512\n",
      "epoch = 200, weight = 1.9427181482315063, loss = 0.007050529122353\n",
      "epoch = 250, weight = 1.9466794729232788, loss = 0.006109020207077\n",
      "epoch = 300, weight = 1.9503668546676636, loss = 0.005293296184391\n",
      "epoch = 350, weight = 1.9537992477416992, loss = 0.004586515948176\n",
      "epoch = 400, weight = 1.9569942951202393, loss = 0.003974071703851\n",
      "epoch = 450, weight = 1.9599684476852417, loss = 0.003443434368819\n",
      "epoch = 500, weight = 1.9627368450164795, loss = 0.002983632031828\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epoch = 500\n",
    "# Replacing manual loss function with nn module\n",
    "# The nn package also contains definitions of popular loss functions\n",
    "# Loss function is callable; recieves similar arguements as before i.e. (y_pred, y)\n",
    "loss = nn.MSELoss()\n",
    "# Use the optim package to define an Optimizer that will update the weights of the model for us.\n",
    "# Here we will use SGD(stochastic gradient descent); the optim package contains many other optimization algorithms. \n",
    "# The first argument to the SGD constructor tells the optimizer which Tensors it should update.\n",
    "# torch.optim: Contains optimizers such as SGD, which update the weights of Parameter during the backward step\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    # Calling forward function since the froward operation is now happeing with Linear transformation\n",
    "    output_pred = model(input_data)\n",
    "    # Compute and print loss\n",
    "    ls = loss(output_pred, output_data)\n",
    "    if i%50==49:\n",
    "        # unpack the weight and bais\n",
    "        [w,b] = model.parameters()\n",
    "        # this will be lists of list so using w[0][0]\n",
    "        print(f\"epoch = {i+1}, weight = {w[0][0].item()}, loss = {ls:.15f}\")\n",
    "    # Before the backward pass, use the optimizer object to zero all of the gradients for the variables \n",
    "    # it will update (which are the learnable weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward() is called.\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    ls.backward()\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters(weights)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: f(15) = 29.642254\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction after training: f(15) = {model(test).item():5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our **Custom nn Module** by defining class having constructor and forward method, for details see **[here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules)** Below is the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Linear(in_feature, out_feature)\n",
    "\n",
    "# Module: creates a callable which behaves like a function, but can also contain state(such as neural net layer weights). \n",
    "# It knows what Parameter (s) it contains and can zero all their gradients, loop through them for weight updates, etc.\n",
    "class custom_linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(custom_linear,self).__init__()\n",
    "        # In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        # define layers\n",
    "        self.lin1 = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # In the forward function we accept a Tensor of input data and we must return a Tensor of output data. \n",
    "        # We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        return self.lin1(x)\n",
    "\n",
    "model = custom_linear(in_feature,out_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
